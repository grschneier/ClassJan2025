{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Dataset Integration with pandas (Marketing Analytics)\n",
    "\n",
    "This notebook demonstrates a practical, repeatable workflow to **clean** messy tabular data and **combine** multiple sources into a single, analysis-ready dataset—exactly the kind of preparation work that powers reliable **campaign reporting**, **funnel analysis**, and **performance dashboards**.\n",
    "\n",
    "**Tools:** Python, pandas  \n",
    "**Core skills:** data quality checks, NULL handling, type casting, deduplication, aggregation, union (concat), joins (merge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project goals\n",
    "\n",
    "By the end of this notebook, you will have a **tidy, analysis-ready table** created from multiple raw files. Along the way, we:\n",
    "\n",
    "- Improve data quality:\n",
    "  - Handle missing values (NULLs)\n",
    "  - Create calculated fields\n",
    "  - Convert columns to `datetime`\n",
    "  - Remove duplicate rows and validate row counts\n",
    "- Combine datasets in the ways analysts use most often:\n",
    "  - Aggregate with `groupby()` for KPIs\n",
    "  - UNION tables with `concat()` (stacking similar datasets)\n",
    "  - JOIN tables with `merge()` (connecting keys across sources)\n",
    "- Produce a small set of **marketing-friendly outputs** (clean tables + summary metrics) that are ready for visualization or modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pandas\n",
    "---\n",
    "First things first, include your imports at the top of your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:05.988004Z",
     "start_time": "2024-12-04T15:51:05.704305Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "---\n",
    "\n",
    "Start by reading in the orders csv file in the datasets folder. We'll use the orders table from Super Store for the first set of tasks - you're already familiar with this dataset from working in SQL, now let's analyze it in Python!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T17:29:18.711649Z",
     "start_time": "2024-12-04T17:29:18.403940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders = pd.read_csv('orders.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "---\n",
    "Start with some exploratory analysis methods to inspect the data. Which of the methods we've learned so far should we always remember to use when working with a new dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:06.968020Z",
     "start_time": "2024-12-04T15:51:06.961615Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.122733Z",
     "start_time": "2024-12-04T15:51:06.999654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning is the process of assembling data into a usable format for analysis.\n",
    "\n",
    "Common data cleaning actions include: \n",
    "- Reformatting dates so that Python recognizes them.\n",
    "- Extracting day/hour/month/year from a date to aggregate by those categories.\n",
    "- Removing duplicate values or rows.\n",
    "- Combining data sources into one table.\n",
    "- Concatenating or separating data.\n",
    "\n",
    "\n",
    "\n",
    "The data sets that we receive as analysts are often very messy, but there’s no need to be fazed by them. While we can’t possibly cover every single cleaning function in this course, there are tons of resources out there about cleaning functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Primary Strategies for Handling NULLs\n",
    "---\n",
    "\n",
    "Finding missing data is the easy part! Determining what to do next is more complicated. Typically, we’re most interested in knowing why we are missing data. Once we know the “type of missingness” (i.e., the source or cause of missing data), we can proceed effectively. This is essential to deciding whether to delete incomplete values or fill them in and, if so, with what.\n",
    "\n",
    "Recap: **A NULL value is any missing value in your data.**\n",
    "\n",
    "One common way of conceptualizing a NULL value is thinking of it as “empty” — not zero, not the word “NULL,” but simply empty.\n",
    "\n",
    "\n",
    "1. Using **external references**, find the true value of the missing data and fill it in using `df['column'].fillna(value)`.\n",
    "2. **Fill with some value:** we have a few options here!\n",
    "    - Impute (i.e., fill in) missing values with the mean, median, or some other calculated value. For example: `.df['column'].fillna(df['column'].mean())`\n",
    "    - Fill with specific values: If you think you know what the values should be, you can replace a value with something else to standardize datasets using `df.replace(value_old, value_new)`\n",
    "    - Fill with interpolated values: pandas has a `.interpolate()` method which will automatically calculate missing values based on linear calculations.\n",
    "3. **Ignore them:** if the missing data wasn't relevant to your analysis, it's okay to leave the NULLs where they are. \n",
    "4. **Remove rows** containing NULL values: For some analyses, rows without information in an important column are entirely useless. Use `df.dropna(subset=['column'])` to remove them. _A general rule of thumb is to not remove more than 5% of your dataset using this method._\n",
    "\n",
    "For the last two options listed here, proceed with caution! These can rely on dangerous assumptions and are usually not good approaches!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.isnull()` and `.sum()` functions to count the nulls in each column. Chain on the `.sort_values()` function to find the columns with the largest number of nulls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.410073Z",
     "start_time": "2024-12-04T15:51:07.302264Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a small number of nulls in our region ID column. Let's drop these rows from our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.624568Z",
     "start_time": "2024-12-04T15:51:07.494748Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.703765Z",
     "start_time": "2024-12-04T15:51:07.659287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.dropna(subset = ['region_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our IT team confirmed that all missing postal code values should be 10001. Let's fill the null values using a built-in pandas function called `.fillna()`. This function will fill all the null values with a specified replacement, and it accepts the `inplace = True` statement to make our changes stick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.774597Z",
     "start_time": "2024-12-04T15:51:07.771567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders['postal_code'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.869134Z",
     "start_time": "2024-12-04T15:51:07.865891Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#orders['postal_code'].fillna(10001).isnull().sum()\n",
    "orders['postal_code'].fillna(10001, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the sum of nulls in each column should now show that our dataset is null-free:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:07.958246Z",
     "start_time": "2024-12-04T15:51:07.955098Z"
    }
   },
   "outputs": [],
   "source": [
    "orders['postal_code'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:08.260548Z",
     "start_time": "2024-12-04T15:51:08.256799Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders['postal_code'].fillna(10001, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:08.381553Z",
     "start_time": "2024-12-04T15:51:08.378513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders['postal_code'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding calculated values to our dataset\n",
    "---\n",
    "\n",
    "Many of our cleaning operations involve applying an operation to a Series. This caan be used to create new columns based on existing data.\n",
    "\n",
    "For this example, let's write a profit_margin function that accepts a row of data, which is a dictionary. It should return the result of dividing the profit column by the sales column (i.e. profit/sales).\n",
    "\n",
    "To start, we'll need to create a function that divides a profit value by a sum value for a single row of a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:08.426005Z",
     "start_time": "2024-12-04T15:51:08.424182Z"
    }
   },
   "outputs": [],
   "source": [
    "def profit_margin(row):\n",
    "    return row['profit'] / row['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:52:27.425595Z",
     "start_time": "2024-12-04T15:52:23.702194Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.apply(profit_margin, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:52:30.415334Z",
     "start_time": "2024-12-04T15:52:30.411460Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders['Kihoon'] = orders['profit'] / orders['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:52:34.129265Z",
     "start_time": "2024-12-04T15:52:34.122792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this function directly to every row in our orders DataFrame at once using a nifty pandas function called `.apply()`. \n",
    "\n",
    "This will pass each row as an individual piece of input into our profit margin function and append the output of our function to a new column. \n",
    "\n",
    "This output will be whatever our function **returns**, so be careful not to use a print statement instead. The default output of a function that is missing a return statement will be a NoneType object, which will often appear as just the word \"None\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.apply()`, create a new column in the orders dataframe called 'profit_margin' by applying the profit margin function row-by-row. Run this using the parameter `axis=1` to apply a function to each row one at a time. Note that the error returned by setting `axis=0` is a KeyError.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:11.329092Z",
     "start_time": "2024-12-04T15:51:11.323991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:13.800199Z",
     "start_time": "2024-12-04T15:51:11.371172Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.apply(profit_margin, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:16.174644Z",
     "start_time": "2024-12-04T15:51:13.819449Z"
    }
   },
   "outputs": [],
   "source": [
    "orders['profit_margin'] = orders.apply(profit_margin, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:16.257850Z",
     "start_time": "2024-12-04T15:51:16.252723Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you\n",
    "\n",
    "We also want to create a column called `margin_categorization` that categorizes our new profit margin as \"profitable\", \"unprofitable\", or \"break even\". We can create a new function to do this, and then apply it to our orders DataFrame using the same method as above. \n",
    "\n",
    "The function should follow these rules:\n",
    "- If the profit_margin is greater than 0, the function should return \"profitable\"\n",
    "- If the profit_margin is 0, the function should return \"break even\"\n",
    "- If the profit_margin is less than 0, the function should return \"unprofitable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:16.298593Z",
     "start_time": "2024-12-04T15:51:16.296576Z"
    }
   },
   "outputs": [],
   "source": [
    "def margin_categorization(kihoon):\n",
    "    if kihoon['profit_margin'] >0:\n",
    "        return \"profitable\"\n",
    "    elif kihoon['profit_margin'] == 0:\n",
    "        return \"break even\"\n",
    "    else:\n",
    "        return \"unprofitable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:18.175730Z",
     "start_time": "2024-12-04T15:51:16.388175Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.apply(margin_categorization, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the new `margin_category` column to find how many orders in the dataset were unprofitable. There are multiple ways to do this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.045117Z",
     "start_time": "2024-12-04T15:51:18.269942Z"
    }
   },
   "outputs": [],
   "source": [
    "orders['margin_category'] = orders.apply(margin_categorization, axis=1)\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Dates and Times\n",
    "---\n",
    "\n",
    "Some of the most challenging, frequently ill-formatted types of data are dates and times. Fortunately, pandas is on top of it with the `.to_datetime()` method.\n",
    "\n",
    "Once a Series has been given a datetime data type, we can use access methods to extract specific time properties, like day or hour.\n",
    "We can also use the pandas `Timestamp()` method to convert data into timestamps:\n",
    "`pd.Timestamp(date_string_or_number)`\n",
    "\n",
    "Convert the ship_date column to a datetime object using `pd.to_datetime()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.121730Z",
     "start_time": "2024-12-04T15:51:20.116304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.337188Z",
     "start_time": "2024-12-04T15:51:20.177159Z"
    }
   },
   "outputs": [],
   "source": [
    "# THIS IS \"WET\" CODE, MAKE IT \"DRY\"\n",
    "\n",
    "orders['ship_date'] = pd.to_datetime(orders['ship_date'], dayfirst=True)\n",
    "orders['order_date'] = pd.to_datetime(orders['order_date'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.512320Z",
     "start_time": "2024-12-04T15:51:20.509577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:29:31.156495Z",
     "start_time": "2024-12-04T16:29:31.154218Z"
    }
   },
   "outputs": [],
   "source": [
    "# stretch exercise - create a function to apply datetime conversion into all the related columns.\n",
    "def makeDT(row):\n",
    "    return row['order_date'] = pd.to_datetime(row['order_date'], dayfirst=True)\n",
    "\n",
    "\n",
    "orders.apply(makeDT, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Duplicates\n",
    "---\n",
    "\n",
    "Fortunately, the issue of duplicate data is a mere pandas method away from being solved! We can use the `.drop_duplicates()` method.\n",
    "\n",
    "If we want to drop duplicates based on certain columns, we can do that, too: `df.drop_duplicates(subset=['column_a', 'column_b'])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.652938Z",
     "start_time": "2024-12-04T15:51:20.643583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.727984Z",
     "start_time": "2024-12-04T15:51:20.717517Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders[['ship_mode', 'customer_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:20.931282Z",
     "start_time": "2024-12-04T15:51:20.844937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders[['ship_mode', 'customer_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:21.070815Z",
     "start_time": "2024-12-04T15:51:20.996885Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.drop_duplicates(subset=['ship_mode', 'customer_id'])[['product_id', 'sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:23.265873Z",
     "start_time": "2024-12-04T15:51:22.789561Z"
    }
   },
   "outputs": [],
   "source": [
    "orders.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unioning DataFrames\n",
    "---\n",
    "\n",
    "We can combine or concatenate two DataFrames together with the `pd.concat()` method. This gives us the option to stack the DataFrames vertically to add more rows, or add the DataFrames side by side as new columns. \n",
    "\n",
    "**Note:** There are a lot of parameters that can be used to control how datasets are unioned using `pd.concat()`, including whether you allow duplicate entries and whether you're concatenating axes that are not shared between the DataFrames. Be sure to read the documentation if you want to use this method!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:23.324404Z",
     "start_time": "2024-12-04T15:51:23.322070Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])\n",
    "df2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:23.385942Z",
     "start_time": "2024-12-04T15:51:23.383244Z"
    }
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:23.490308Z",
     "start_time": "2024-12-04T15:51:23.487242Z"
    }
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:23.581937Z",
     "start_time": "2024-12-04T15:51:23.578582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gives more rows but note that index is repeated!\n",
    "pd.concat([df1, df2], join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.001898Z",
     "start_time": "2024-12-04T15:51:24.998404Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], join = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.062863Z",
     "start_time": "2024-12-04T15:51:25.058971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gives more columns but note that column names are repeated!\n",
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen a small example, let's apply this to the orders DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.185168Z",
     "start_time": "2024-12-04T15:51:25.182214Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.424544Z",
     "start_time": "2024-12-04T15:51:25.348742Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, make small samples of the DataFrame that we can use for this example\n",
    "orders_2016 = orders[orders['ship_date'].dt.year == 2016]\n",
    "orders_2017 = orders[orders['ship_date'].dt.year == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.526306Z",
     "start_time": "2024-12-04T15:51:25.520698Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.612285Z",
     "start_time": "2024-12-04T15:51:25.606317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.714913Z",
     "start_time": "2024-12-04T15:51:25.711063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.shape, orders_2016.shape, orders_2017.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pd.concat` by passing a **list of the dataframes you want to concatenate** as an argument. Save this concatenated version as a new DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:25.951223Z",
     "start_time": "2024-12-04T15:51:25.944597Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list = [orders_2016, orders_2017]\n",
    "\n",
    "df_concat = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:26.054168Z",
     "start_time": "2024-12-04T15:51:26.013783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat_2 = pd.concat(df_list, axis=1)\n",
    "df_concat_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:26.161971Z",
     "start_time": "2024-12-04T15:51:26.154317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the index values above don't match the total number of rows printed at the bottom of our newly unioned DataFrame! When you concatenate or union two DataFrames together, you'll need to reset the index if you want it to be cleanly number from 0 to the maximum number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:26.218643Z",
     "start_time": "2024-12-04T15:51:26.197115Z"
    }
   },
   "outputs": [],
   "source": [
    "df_concat.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames\n",
    "---\n",
    "\n",
    "JOINing is the process of combining DataFrames according to specific values. Traditionally, this would be done with SQL - we already know how to do this!\n",
    "\n",
    "JOINing allows us to:\n",
    "- Reduce the size of a database.\n",
    "- Increase the speed at which data is queried and returned.\n",
    "- Reduce the redundancy of the data stored in the database.\n",
    "- Access data that is split across multiple tables\n",
    "\n",
    "**Recap:** A JOIN relies on multiple data sets that share a common unique identifier, or \"key\".\n",
    "\n",
    "JOIN is used to combine tables for the purpose of adding selection criteria and possibly additional columns. \n",
    "- They connect data sources together in order to use information from both tables to display a desired result.\n",
    "- A JOIN allows for tables to be connected using common columns, which serve as unique identifiers (called KEYS).\n",
    "- Note that unique identifiers aren’t required, but are almost always used to avoid unintended behavior.\n",
    "\n",
    "The robust method for JOINing in pandas is `merge()`, which accepts several parameters:\n",
    "\n",
    "`pd.merge(left_df, right_df, how, left_on, right_on)`\n",
    "\n",
    "As you may have guessed, the first two parameters are the DataFrames to JOIN. The third parameter describes the type of JOIN, typically “left.” The last two parameters provide the column name for the shared column, or foreign key, that will be used to combine the two DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice joining datasets by loading in the products, returns, and regions csv files from the datasets folder. These will all be familiar from our time spent in pgAdmin!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T15:51:26.801752Z",
     "start_time": "2024-12-04T15:51:26.405670Z"
    }
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('datasets/products.csv', encoding='unicode_escape')\n",
    "returns = pd.read_csv('datasets/returns.csv')\n",
    "regions = pd.read_csv('datasets/regions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to join the products and orders dataframes. Explore both dataframes to identify the common column between them. Use a left join to combine the tables in a dataframe named `orders_with_products`.\n",
    "\n",
    "First, print a clean DataFrame that shows all of the column names and data types for the orders table. Do the same for the products table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(orders.dtypes, columns=['DataTypes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(products.dtypes, columns=['DataTypes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables both have a column called `product_id` that we can use as a key to join on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_products=pd.merge(left = products, right = orders, how='left', on= 'product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_with_products.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `pd.merge` again to join the returns table onto our table called `orders_with_products`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `order_id` column as the key to join on here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=orders_with_products, right = returns, how ='left', on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_products_returns =pd.merge(left=orders_with_products, right = returns, how ='left', on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders_with_products_returns.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now you:\n",
    "\n",
    "Finally, join the region dataset to the combined dataframe above. Determine which column to use as a key and name your final DataFrame `combined_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(left=orders_with_products_returns, right=regions, how='left', on='region_id')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating with groupby()\n",
    "---\n",
    "In pandas, `groupby()` statements allow us to segment our population to a specific subset and draw calculations based on those segment. A basic example looks like this:\n",
    "`data_frame.groupby(['column_a']).count()`\n",
    "\n",
    "We can think about a `groupby()` statement in three steps:\n",
    "- Split: Separate our DataFrame into groups according to a specific attribute.\n",
    "- Apply: Apply some function to the groups, like sum, count, or max.\n",
    "- Combine: Put our DataFrame back together and check the output.\n",
    "\n",
    "\n",
    "We can use the .agg() method to get multiple aggregate values: \n",
    "\n",
    "`df.groupby('col_a')['col_b'].agg(['count', 'mean', 'min', 'max'])`\n",
    "\n",
    "The above command does the following:\n",
    "- Takes our DataFrame, `df`\n",
    "- Groups it by the values in `col_a` (\"Split\")\n",
    "- Calculates the count, mean, minimum, and maximum of `col_b` (\"Apply\")\n",
    "- And combines the results in a Series or DataFrame that's easy for us to digest (\"Combine\")\n",
    "\n",
    "We can also groupby() multiple columns to drill down further:\n",
    "`df.groupby(['first_column', 'second_column'])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run through a couple of examples of groupby using this dataset. \n",
    "\n",
    "Segment the orders DataFrame and explore aggregate values to answer the following questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which discount results in the highest mean order quantity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, group by discount and take the mean of the quantity column\n",
    "# orders.groupby('discount')\n",
    "# orders.groupby('discount')['quantity']\n",
    "# orders.groupby('discount')['quantity'].count()\n",
    "# orders.groupby('discount')['quantity'].min()\n",
    "# orders.groupby('discount')['quantity'].max()\n",
    "orders.groupby('discount')['quantity'].mean().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain on a sort_values command to sort in DESCENDING order\n",
    "orders.groupby('discount')['quantity'].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the index to get the discount values sorted by largest mean quantity\n",
    "orders.groupby('discount')['quantity'].mean().sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first discount value - this will be associated with the largest mean quantity!\n",
    "orders.groupby('discount')['quantity'].mean().sort_values(ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining commands like shown above can answer complex questions with just one line of code. It can be hard to read when you're just getting started learning Python, so don't be afraid to break it out across cells and see what your output looks like each step of the way! Take advantage of how easy it is to index DataFrames, Series, lists, and so on with our square brackets []. \n",
    "\n",
    "Let's answer another question: Which product has the highest mean discount applied?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Kt5tysEvpBC"
   },
   "outputs": [],
   "source": [
    "# First, group by product ID and find the average discount\n",
    "orders.groupby('product_id')['discount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain on a sort_values command to sort in DESCENDING order\n",
    "orders.groupby('product_id')['discount'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.groupby('product_id')['discount'].mean().sort_values(ascending=False)['OFF-AP-10002899']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the index to get the product ID sorted by largest mean discount\n",
    "orders.groupby('product_id')['discount'].mean().sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, select the first product ID in this index - this will be associated with the largest mean discount!\n",
    "orders.groupby('product_id')['discount'].mean().sort_values(ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply our group by method to the combined DataFrame `combined_df` we created when joining tables together. Let's use this to determine the salesperson who is generating the most profit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which salesperson is generating the most profit\n",
    "candidate_cols = [c for c in combined_df.columns if any(k in c.lower() for k in ('salesperson','sales_rep','sales_person','employee','rep'))]\n",
    "if not candidate_cols:\n",
    "    raise KeyError(f\"No salesperson-like column found in combined_df. Columns: {combined_df.columns.tolist()}\")\n",
    "\n",
    "sales_col = candidate_cols[0]\n",
    "profit_by_sales = combined_df.groupby(sales_col)['profit'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Top salesperson and their total profit\n",
    "top_salesperson = profit_by_sales.index[0]\n",
    "print(top_salesperson, profit_by_sales.iloc[0])\n",
    "\n",
    "# Optionally view top 5\n",
    "print(profit_by_sales.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "---\n",
    "Today, we:\n",
    "\n",
    "- Used pandas to handle missing or problematic data values.\n",
    "- Identified appropriate cleaning strategies for specific types of data.\n",
    "- Used groupby() and JOIN statements to combine data with pandas.\n",
    "- Created insights from data by splitting and combining data segments.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6_09_Cleaning_and_Combining_Data_with_pandas_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
